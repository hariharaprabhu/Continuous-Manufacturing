{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting output sequence in Multi-Stage Continuous Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This problem refer the dataset obtained from kaggle (Provided by Liveline)\n",
    "\n",
    "\n",
    "https://www.kaggle.com/supergus/multistage-continuousflow-manufacturing-process\n",
    "\n",
    "##  Tasks achieved:\n",
    "\n",
    "### The initial goal over here is to predict the output sequence \n",
    "     \n",
    "     1)  There are about 28 characteristics that needs to be predicted\n",
    "     2)  The ML model should determine what characteristics can be predicted based on the input sequence\n",
    "     3)  For convenience used Auto-ML library TPOT  that helps in hyper-parameter tuning the model for \n",
    "         each characteristic more information about tpot can be found at \n",
    "     4)  Currently, a regression based approach is used to predict output sequence\n",
    "     5)  After predicting the output signal, the actual and predicted outputs are compared with the warning limit                        setting to identify the good and bad signals\n",
    "     6)  Accuracy metric MAE has been used to identify the optimum model \n",
    "     7)  The model files and normalization files are stored as joblib file that helps in deployment\n",
    "     8)  The feature importance plot helps to identify what characteristic could help in predicting the sequence\n",
    "     9)  The accuracy metric r2 helps to explain how much variability can be explained by the sequence\n",
    "     10) The residual plot helps to understand which models are working well\n",
    "\n",
    "\n",
    "##   More to Come:\n",
    "      \n",
    "      1) Output needs to be discussed with clients to see if it makes sense or any logic needs to be added\n",
    "      2) This problem needs to be converted to time-series format to predict the upcoming  sequence \n",
    "      3) A classification model could be developed to predict if a input sequence is good or bad\n",
    "      4) As of now, the extreme outliers are removed from consideration. But if our focus is on outliers, anomaly detection   \n",
    "         models could be applied (including Auto-Encoders, Isolation Forest)\n",
    "      5) A flask based app can be deployed in Heroku (or) any private cloud that helps to share results with customers and for\n",
    "         deploying models in real-time\n",
    "      6) The ML model provided here can be used as a baseline . A Deep learning based (LSTM, CNN) based approach needs to be \n",
    "         tested to see if accuracy can be improved further\n",
    "      7) Need to include some message brokers strategies to deploy a real-time solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Dependencies\n",
    "    Identify the output fields and the threshold limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask  # For Parallel processing\n",
    "from tpot import TPOTRegressor # Auto-ML\n",
    "from sklearn.model_selection import train_test_split, KFold # Stratified train, test split and cross validation\n",
    "import os # To set path\n",
    "from sklearn.preprocessing import StandardScaler  # Normalization techniques\n",
    "from sklearn.metrics import mean_squared_error,r2_score # Accuracy metrics\n",
    "from joblib import dump, load  # Loading and dumping model files\n",
    "import matplotlib.backends.backend_pdf  # Adding plots to pdf\n",
    "import matplotlib.pyplot as plt # Creating Plot\n",
    "import seaborn as sns # Creating Plot\n",
    "import math # For Calculating rmse\n",
    "import warnings # For ignoring warnings from pandas\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df = pd.read_csv(\"continuous_factory_process.csv\")\n",
    "\n",
    "output_col = df.filter(regex = 'Output').columns.copy()\n",
    "setpoint_col = df.filter(regex = 'Setpoint').columns.copy()\n",
    "\n",
    "output_col= sorted(set(output_col) - set(setpoint_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a variable called Setting based on RawMaterial Properties to stratify train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_material_properties = list(df.filter(regex = 'RawMaterial.Property').columns.copy())\n",
    "unique_comb_parameters = df.groupby(raw_material_properties).size().reset_index().rename(columns={0:'count'})\n",
    "unique_comb_parameters.index.name = 'Setting'\n",
    "unique_comb_parameters.drop('count', axis=1, inplace=True)\n",
    "unique_comb_parameters = unique_comb_parameters.reset_index()\n",
    "unique_comb_parameters['Setting'] = 'S'+ unique_comb_parameters['Setting'].astype(str)\n",
    "#df.groupby(raw_material_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the setting variable with the dataframe and identify the input columns for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, unique_comb_parameters, on = raw_material_properties, how = 'left')\n",
    "input_col = df.columns.drop(df.filter(regex='Output|Setpoint|time_stamp').columns).copy()\n",
    "input_col = list(input_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing, Train & test model, store model files and test effectiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filt_df = df[input_col]\n",
    "\n",
    "winning_pipes = []\n",
    "scores = []\n",
    "cross_val_scores = pd.DataFrame(columns = ['CHARACTERISTIC_NAME', 'R2_score', 'MAE', 'RMSE'])\n",
    "selected_features = pd.DataFrame(columns = ['CHARACTERISTIC_NAME', 'INPUT_FEATURES'])\n",
    "\n",
    "\n",
    "# Define the directory location here ex: C:/Test/Feature_Imp_Plot.pdf\n",
    "feat_imp_plot = matplotlib.backends.backend_pdf.PdfPages(\"Feature_Imp_Plot_v2.pdf\")\n",
    "line_plot = matplotlib.backends.backend_pdf.PdfPages(\"Line_Plot_v2.pdf\")\n",
    "resid_plot = matplotlib.backends.backend_pdf.PdfPages(\"resid_plot_v2.pdf\")\n",
    "\n",
    "\n",
    "# Predict the output sequence for each sequence\n",
    "for col in output_col:\n",
    "    overall_col = input_col.copy()\n",
    "    overall_col.append(col)\n",
    "    filt_df = df[overall_col]\n",
    "    \n",
    "    filt_df = filt_df.loc[filt_df[col]>0].copy() # many records with 0 as output value are removed\n",
    "    spec_limit = filt_df.groupby(['Setting']).agg({col:{'Mean':'mean', 'Std':'std'}})\n",
    "    spec_limit.columns = spec_limit.columns.droplevel(0) \n",
    "    spec_limit = spec_limit.reset_index()\n",
    "    \n",
    "    # For removing outliers +/- 3sigma limits are used to remove extreme outliers for each setting\n",
    "    new_df = pd.merge(filt_df, spec_limit, on = ['Setting'], how = 'inner')\n",
    "    new_df  = new_df.loc[(new_df[col] > new_df['Mean'] - 3* new_df['Std'])\n",
    "                             & (new_df[col] < new_df['Mean'] + 3* new_df['Std'])].copy()\n",
    "\n",
    "    new_df = new_df.drop(['Mean', 'Std'], axis=1)\n",
    "    \n",
    "    \n",
    "    # Filter Input Factors based on corr value with output\n",
    "    corr_matrix = new_df.corr().abs()\n",
    "    corr_matrix = corr_matrix.loc[corr_matrix.index == col].copy()\n",
    "    filt_col = [c for c in corr_matrix.columns if any(corr_matrix[c] > 0.3)] # Filter columns with higher correlation\n",
    "    \n",
    "    \n",
    "    # Split the dataset into training and testinng stratified based on setting parameter\n",
    "    stratified_col = filt_col.copy()\n",
    "    stratified_col.append('Setting')\n",
    "    new_df = new_df[stratified_col]\n",
    "    new_df = new_df.dropna()\n",
    "    \n",
    "    filt_df = new_df.loc[new_df[col]>1].copy()\n",
    "    \n",
    "    if (isinstance(filt_df, pd.DataFrame)):\n",
    "        if(len(filt_df)>1):\n",
    "            y = new_df['Setting']\n",
    "            # Stratify training and testing\n",
    "            train_x, test_x, train_y, test_y = train_test_split(new_df, new_df[col], test_size = 0.25,stratify = y, shuffle = True)\n",
    "            train_x = train_x.drop(['Setting', col], axis = 1)\n",
    "            test_x = test_x.drop(['Setting', col], axis = 1)\n",
    "            \n",
    "            # Apply normalization techniques to perform standardization\n",
    "            sc = StandardScaler()\n",
    "            train_x_trans = sc.fit_transform(train_x)\n",
    "            test_x_trans = sc.transform(test_x)\n",
    "    \n",
    "            #print (\"Number of Records = \", len(train_x))\n",
    "            \n",
    "            \n",
    "            cv = KFold(n_splits=3)\n",
    "            cv_iter = list(cv.split(train_x_trans, train_y))\n",
    "            tpot = TPOTRegressor(verbosity = 1, random_state = 15, n_jobs = -1, generations = 10, population_size = 50, \n",
    "                                 memory = None,\n",
    "                                 cv=cv_iter,use_dask = True)\n",
    "            # Tpot will use dask to parallel process multiple model, overall 600 different models are expeted\n",
    "\n",
    "            tpot.fit(train_x_trans, train_y)\n",
    "            winning_pipes.append(tpot.fitted_pipeline_) # Best Pipeline is stored to a .py file\n",
    "\n",
    "            scores.append(tpot.score(test_x_trans, test_y)) # Test Scores are stored\n",
    "            \n",
    "            \n",
    "            # The selected features corresponding to each output filed are stored and also cross val scores are stored\n",
    "            selected_features = selected_features.append({'CHARACTERISTIC_NAME': col, 'INPUT_FEATURES': list(train_x.columns)}, ignore_index = True)\n",
    "            cross_val_scores = cross_val_scores.append({'CHARACTERISTIC_NAME': col, 'R2_score': r2_score(train_y, tpot.predict(train_x_trans)),\n",
    "                                                        'MAE':mean_squared_error(test_y, tpot.predict(test_x_trans)),\n",
    "                                                        'RMSE':math.sqrt(mean_squared_error(test_y, tpot.predict(test_x_trans)))\n",
    "                                                       }, ignore_index=True)\n",
    "            #print (\"Cross Val Score = \", cross_val_scores)\n",
    "            \n",
    "            \n",
    "            model_directory = os. getcwd() \n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            model_file = model_directory+ col+ \".py\"\n",
    "            norm_file = model_directory+ col+ \"norm\"+ \".joblib\"\n",
    "            model_joblib = model_directory+ col+ \".joblib\"\n",
    "            tpot.export(model_file)\n",
    "            \n",
    "            # Store the model files and norm files \n",
    "            dump(tpot.fitted_pipeline_, model_joblib)\n",
    "            dump(sc,norm_file)\n",
    "            \n",
    "            # Accuracy Metrics\n",
    "            r2_val = r2_score(test_y, tpot.predict(test_x_trans))\n",
    "            mae_val = mean_squared_error(test_y, tpot.predict(test_x_trans))\n",
    "            \n",
    "            \n",
    "            \n",
    "            train_x['Predicted'] = tpot.predict(train_x_trans)\n",
    "            test_x['Predicted'] = tpot.predict(test_x_trans)\n",
    "            train_x['Output'] = train_y#tpot.predict(X_test)\n",
    "            test_x['Output'] = test_y#tpot.predict(X_test)\n",
    "\n",
    "            \n",
    "            limit_col = col.replace('Actual', 'Setpoint')\n",
    "            max_limit = df[limit_col].max()\n",
    "            \n",
    "            train_x['Set_Point'] = max_limit\n",
    "            train_x['Actual_Warning'] = 0\n",
    "            train_x['Predicted_Warning'] = 0\n",
    "            train_x.loc[train_x['Output']>max_limit, 'Actual_Warning'] = 1\n",
    "            train_x.loc[train_x['Predicted']>max_limit, 'Predicted_Warning'] = 1\n",
    "            \n",
    "            test_x['Set_Point'] = max_limit\n",
    "            test_x['Actual_Warning'] = 0\n",
    "            test_x['Predicted_Warning'] = 0\n",
    "            test_x.loc[test_x['Output']>max_limit, 'Actual_Warning'] = 1\n",
    "            test_x.loc[test_x['Predicted']>max_limit, 'Predicted_Warning'] = 1\n",
    "            \n",
    "            training_file = col + \".csv\"\n",
    "            testing_file =  col + \".csv\"\n",
    "\n",
    "            train_x.to_csv(training_file)\n",
    "            test_x.to_csv(testing_file)\n",
    "            \n",
    "            \n",
    "            fig = plt.figure(figsize = (10,5))\n",
    "            extracted_best_model = tpot.fitted_pipeline_.steps[-1][1]\n",
    "            extracted_best_model.fit(train_x[selected_features['INPUT_FEATURES'][0]], train_y)\n",
    "            if hasattr(extracted_best_model, 'coef_'):\n",
    "                feat_importances = pd.Series(extracted_best_model.coef_, index=selected_features['INPUT_FEATURES'][0]).sort_values(ascending = False)\n",
    "                feat_importances.plot(kind='barh', title = col)\n",
    "                plt.show()\n",
    "                feat_imp_plot.savefig(fig,bbox_inches = 'tight',dpi = 100)\n",
    "            if hasattr(extracted_best_model, 'feature_importances_'):\n",
    "                feat_importances = pd.Series(extracted_best_model.feature_importances_, index=selected_features['INPUT_FEATURES'][0]).sort_values(ascending = False)\n",
    "                feat_importances.plot(kind='barh', title = col)\n",
    "                plt.show()\n",
    "                feat_imp_plot.savefig(fig,bbox_inches = 'tight',dpi = 100)\n",
    "            \n",
    "            \n",
    "            sns_plot = sns.lmplot(x='Output', y=\"Predicted\", data=test_x)# hue = \"MACHINE_NUMBER\")\n",
    "            fig = sns_plot.fig\n",
    "            plt.suptitle(col +\"\\n\" + \"R2 Score = \"+ str(round(r2_val,4)) + \" , MSE =\" +str(round(mae_val,4)), fontsize  = 12)\n",
    "            line_plot.savefig(fig,bbox_inches = 'tight',dpi = 100)\n",
    "            #(pd.Series(model.feature_importances_, index=X.columns).nlargest(4).plot(kind='barh'))\n",
    "\n",
    "            \n",
    "line_plot.close()\n",
    "feat_imp_plot.close()\n",
    "selected_features.to_csv(\"Inputfeatures_selected_v3.csv\")\n",
    "cross_val_scores.to_csv(\"Cross_Val_Score_Testing_v3.csv\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
